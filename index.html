<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
<meta http-equiv="content-type" content="text/html;charset=utf-8" />
<meta name="description" content="Topic Models based on Stochastic Block Models" />
<meta name="keywords" content="topic model, NLP, SBM, community detection, complex networks" />
<link rel="shortcut icon" href="paper.ico" />
<link rel="stylesheet" type="text/css" href="main.css" />
<title>TopSBM: Topic Models based on Stochastic Block Models </title>
</head>


<body id="top">


<h1>TopSBM: Topic Models based on Stochastic Block Models </h1>

<h2> Topic modeling with text data </h2>

<p> <a href="https://en.wikipedia.org/wiki/Topic_model" rel="nofollow"
target="_blank"> Topic models </a> are a popular way to extract
information from text data, but its most popular flavours (based on
<a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet</a>
priors, such
as <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">LDA</a>)
make unreasonable assumptions about the data which severely limit its
applicability. Here we explore an alternative way of doing topic
modelling, based
on <a href="https://en.wikipedia.org/wiki/Stochastic_block_model"
rel="nofollow" target="_blank"> stochastic block models (SBM)</a>, thus
exploiting a mathematical connection with
finding <a href="https://en.wikipedia.org/wiki/Community_structure">community
structure</a> in networks.</p>

To briefly illustrate some of the limitations of Dirichlet-based topic
modelling, consider the simple multi-modal mixture of three topics shown
below on the left. Since the Dirichlet distribution is unimodal, it
severely distorts the topics inferred by LDA, as shown in the middle
&mdash; even thought it is just a prior distribution over a
heterogeneous topic mixture. The SBM formulation, on the other hand, can
easily accommodate this kind of heterogeneity, since it is based on more
general priors (see <a href="https://dx.doi.org/10.1103/PhysRevX.4.011047">here</a>, <a href="https://dx.doi.org/10.1103/PhysRevE.95.012317">here</a> and <a href="https://dx.doi.org/10.1103/PhysRevX.5.011033">here</a>).

<p>
<IMG SRC="triangles.png" style="display: block; margin: 0 auto; width: 400px">

  <p>
In addition to this, the SBM method is based a nonparametric “symmetric”
formulation that allows for the simultaneous hierarchical clustering of
documents as well as words. Due to its nonparametric Bayesian nature,
the number of topics in each category, as well as the shape and depth of
the hierarchy, are automatically determined from
the <a href="https://en.wikipedia.org/wiki/Posterior_probability">posterior
distribution</a> according to the statistical evidence available,
avoiding
both <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting and
underfitting</a>.

To illustrate the application of the method using real data, we show
below an example using <a href="https://www.wikipedia.org/">wikipedia</a> articles.

<h3> Example: 63 Wikipedia articles related to Physics</h3>



<IMG SRC="bipartite.png" style="display: block; margin: 0 auto; width: 400px">

The method divides both the documents and words into hierarchical
groups. The divisions found in the first three hierarchical levels can
be inspected below.

	<ul>
	<li> Hierarchy level 1:
	<a href="topsbm_level_0_topics.html"> 21 topics </a>
	(click <a href="topsbm_level_0_topic-dist.html"> here </a> to see topic distribution in each document)
	and <a href="topsbm_level_0_clusters.html"> 5 document groups </a>

	<li> Hierarchy level 2: <a href="topsbm_level_1_topics.html"> 3 topics </a>
	(click <a href="topsbm_level_1_topic-dist.html"> here </a> to see topic distribution in each document)
	and <a href="topsbm_level_1_clusters.html"> 2 document groups </a>

	<li> Hierarchy level 3: <a href="topsbm_level_2_topics.html"> 2 topics </a>
	(click <a href="topsbm_level_2_topic-dist.html"> here </a> to see topic distribution in each document)
	and <a href="topsbm_level_2_clusters.html"> 1 document group </a>
	</ul>


  <h2> Topic modeling beyond just text data </h2>
In many cases, we have additional information available about the documents, such as metadata or hyperlinks.
We can incorporate these different types of data into a unified topic model by extending the above approach using multilayer stochastic block models.
In this approach, each type of data is represented as a different layer in the network.

We can represent the information contained in the Wikipedia dataset with the following three layers:
<ul>
<li> Text: The bipartite network of words and documents as nodes, where edges represent the number of times a word appears in a document  (same as above)
<li> Hyperlinks: The network of documents, where a directed edge corresponds to a link between documents
<li> Metadata: The bipartite network of categories and documents, where an edge represents that a category contains a given document.
</ul>
<IMG SRC="multilayer.png" style="display: block; margin: 0 auto; width: 400px" >

<p>

Using this representation, we can obtain a hierarchical clustering of words (i.e. topics) and documents not only based on the text but taking into account links and metadata information.
We illustrate the differences in the document clustering when taking into account different types of data.
<IMG SRC="multilayer-clustering.png" style="display: block; margin: 0 auto; width: 800px" >


<h2> Installation and usage instructions </h2>
	
<p> Check how to use topSBM in a <a href="TopSBM-tutorial.html"> simple example</a> and in a case <a href="https://github.com/martingerlach/hSBM_Topicmodel/blob/master/Multilayer_SBM_Tutorial.ipynb"> beyond just text data</a>.

<p> The easiest way to run topSBM in your (small) corpus is using the <a href="https://colab.research.google.com/github/edugalt/hSBM_Topicmodel/blob/master/TopSBM-colab.ipynb"> Google Colab tutorial </a>.
	
	
<p> The Python
package <a href="https://graph-tool.skewed.de/">graph-tool</a> is
needed, which also
contains <a href="https://graph-tool.skewed.de/static/doc/demos/inference/inference.html">detailed
documentation on SBM inference</a>.

<p> To run it yourself, you need to:</p>

	<ol>
	  <li>  Install <a href="https://graph-tool.skewed.de/">graph-tool</a>:
	    <ul>
	      <li> see detailed <a href="https://git.skewed.de/count0/graph-tool/wikis/installation-instructions">installation instructions</a>.</li>
	      <li> or using <a href="https://www.docker.com/">Docker</a>: <br> <i> docker pull tiagopeixoto/graph-tool </i> </li>

	      <!-- <li> or using <a href="https://gitlab.com/ostrokach-forge/graph-tool"> Conda</a> (only for linux).</li> -->
	    </ul>
	  </li>

	  <li>  Download the <a href="https://github.com/martingerlach/hSBM_Topicmodel"> package </a> SBM topic models:
	    <ul>
	      <li> Using <a href="https://git-scm.com/"> git </a>: <br> <i> git clone  https://github.com/martingerlach/hSBM_Topicmodel.git </i> </li>
	      <li> or: download <a href="https://github.com/martingerlach/hSBM_Topicmodel/archive/master.zip"> zip file</a>. </li>
	    </ul>
	  </li>

	  <li> Explore the tutorials using <a href="http://www.jupyter.org"> Jupyter </a> notebooks:
	      <ul>
	    <li> <i> jupyter-notebook TopSBM-tutorial.ipynb  </i>  (topsbm for text)  </li>
      <li> <i> Multilayer_SBM_Tutorial.ipynb  </i> (topsbm for different types of data) </li>

	  </li>
	  </ol>


	<h2> References: </h2>

	<ul>
    <li> C. C. Hyland, Y. Tao, L. Azizi, M. Gerlach, T. P. Peixoto, and E. G. Altmann  <em> Multilayer Networks for Text Analysis with Multiple Data Types </em>, <a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00288-5"> EPJ Data Science 10, 33 (2021) </a>.</li>
	  <li> <a href="https://martingerlach.github.io/">M. Gerlach</a>, <a href="https://skewed.de/tiago">T. P. Peixoto</a>, and <a href="http://www.maths.usyd.edu.au/u/ega/">E. G. Altmann</a>, <em> A network approach to topic models, </em>  <a href="http://advances.sciencemag.org/content/4/7/eaaq1360"> Science Advances 4, eaaq1360 (2018) </a> or <a href="https://arxiv.org/abs/1708.01677"> [arXiv:1708.01677] </a>.</li>
	  <li> T. P. Peixoto, <em> Bayesian stochastic blockmodeling  </em> <a href="https://arxiv.org/abs/1705.10225">[arXiv: 1705.10225]</a>.</li>
	</ul>



</body>

</html>
